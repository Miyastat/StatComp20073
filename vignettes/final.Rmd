---
title: "tfinal"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tfinal}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## HW0

Use knitr to produce 3 examples in the book. The 1st example should contain texts and at least one figure. The 2nd example should contains texts and at least one table. The 3rd example should contain at least a couple of LaTeX formulas.

present texts and figures:
```{r HW0}
library(ggplot2)
attach(economics)
plot(date,unemploy)
```

present texts and tables:
```{r}
knitr::kable(head(economics))
```

present a couple of LaTeX formulas
\begin{equation}f(x)=x^{2}\end{equation}
\begin{equation}f(x)=x_{1}^{2}+x_{2}^{2}\end{equation}

## HW1

### EX3.3

The Pareto(a, b) distribution has cdf
$$F(x)=1-\left(\frac{b}{x}\right)^{a}, \quad x \geq b>0, a>0$$
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto(2, 2) distribution. Graph the density histogram of the sample with the Pareto(2, 2) density superimposed for comparison.

```{r HW1}
n <- 1000
u <- runif(n)
x <- (4/(1-u))^0.5
hist(x, prob = TRUE, main = expression(f(x)==8/x^3))
y <- seq(2, 100, .01)
lines(y, 8/y^3)
```

### EX3.9

The rescaled Epanechnikov kernel [85] is a symmetric density function
$$f_{e}(x)=\frac{3}{4}\left(1-x^{2}\right), \quad|x| \leq 1$$
Devroye and Gy¨ orfi [71, p. 236] give the following algorithm for simulation from this distribution. Generate iid U1, U2, U3 ∼ Uniform(−1, 1). If |U3| ≥ |U2| and |U3| ≥ |U1|, deliver U2; otherwise deliver U3. Write a function to generate random variates from fe, and construct the histogram density estimate of a large simulated random sample.

```{r}
n <- 100000
u1 <- runif(n,-1,1)
u2 <- runif(n,-1,1)
u3 <- runif(n,-1,1)
u <- numeric(n)
for (i in 1:n){
  if (abs(u3[i]) >= abs(u2[i]) & abs(u3[i]) >= abs(u1[i])){
    u[i] <- u2[i]
  }else{
    u[i] <- u3[i]
  }
}
hist(u,prob = TRUE, main = expression(f(x) == 3*(1-x^2)/4))
```

### EX3.10

Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_{e}$(3.10).

Let $X_1=|U_1|, X_2=|U_2|, X_3=|U_3|$,then clearly there is $X_1,X_2,X_3 \sim$ Uniform(0,1).  

Let $X= \begin{cases}
X_2 & ~~X_3 \geq X_1~~and~~ X_3 \geq X_2 \\
X_3 & ~~otherwise \\
\end{cases}$  
Then, since $f_e(x)$ is symmetric, all I have to prove is that $X$ obeys the pdf  
$$f_{e'}(x)=2*\frac{3}{4}(1-x^2),~~~~~~~0 \leq x \leq 1$$  

Now, notice that the algorithm to generate $X$ can be rewritten as:  

1. Generate $X_1,X_2,X_3 \sim$ Uniform(0,1).  
2. Remove the largest value $X_{(3)}$  
3. Select one of the remaining two values with equal probability. Let $X$ be this value.  

For any $0 \leq x \leq 1$, consider the events:  
$A=\{$ only one of the $X_i \leq x$ $\}$  
$B=\{$ at least two of the $X_i \leq x$ $\}$,  
Then, it's easy to compute the cdf of $X$, $F_{e'}(x)$ as following:

\begin{align}
F_{e'}(x)&=P(X\leq x)\\
&=P(X \leq x,A)+P(X \leq x,B)\\
&=P(X \leq x~|~A)*P(A) + P(X \leq x~|~B)*P(B)\\
&=\frac{1}{2}*3x(1-x)^2 + 1*[3x^2(1-x) + x^3]\\
&=-\frac{1}{2} x^3 +\frac{3}{2}x,~~~~~~0 \leq x \leq 1
\end{align}  

Thus, the pdf of $X$ is 
$$ f_{e'}(x)=F'_{e'}(x)=2*\frac{3}{4}(1-x^2),~~~~~~~0 \leq x \leq 1 $$  

Thus, the algorithm given in Exercise 3.9 generates variates from the density $f_e. ~~~ \Box$ 

### EX3.13

It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf
$$F(y)=1-\left(\frac{\beta}{\beta+y}\right)^{r}, \quad y \geq 0$$
(This is an alternative parameterization of the Pareto cdf given in Exercise 3.3.) Generate 1000 random observations from the mixture with r = 4 and β = 2. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

```{r}
n <- 1000
u <- runif(n)
x <- 2/(1-u)^(1/4)-2
hist(x, prob = TRUE, main = expression(f(x) == 2^6/(2+x)^5))
y <- seq(0, 100, .01)
lines(y, 2^6/(2+y)^5)
```

## HW2

### EX5.1

Compute a Monte Carlo estimate of
$$
\int_{0}^{\pi / 3} \sin t d t
$$
and compare your estimate with the exact value of the integral.

$$
\int_{0}^\frac{\pi}{3} sin t d t=\frac{\pi}{3} E\left[sinx\right], X \sim U(0,\frac{\pi}{3})
$$

```{r HW2}
 m <- 1e4; x <- runif(m, min = 0, max = pi / 3)
 theta.hat <- mean(sin(x)) * (pi / 3)
 print(c(theta.hat, 0.5))
```

### EX5.7 
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

```{r}
## antithetic variate approach
m <- 1000; x <- runif(m)
theta.hat1 <- mean(exp(x))
theta.hat1

## simple Monte Carlo method
n <- 1000; x <- runif(n)
y1 <- exp(x); y2 <- exp(1-x)
theta.hat2 <- mean((y1+y2)/2)
theta.hat2

print(c(theta.hat1, theta.hat2, exp(1) - 1))

## compare variance of the two methods
c(sd(exp(x)), sd((y1+y2)/2))
```

### EX5.11 
If $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are unbiased estimators of $\hat{\theta}$, and $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are antithetic, we derived that $c^{*}=1/2$ is the optimal constant that minimizes the variance of $\hat{\theta}_{c}=c \hat{\theta}_{2}+(1-c) \hat{\theta}_{2}$. Derive $c^{*}$ for the general case. That is, if $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are any two unbiased estimators of $\theta$, find the value $c^{*}$ that minimizes the variance of the estimator $\hat{\theta}_{c}=c\hat{\theta}_{2}+(1-c) \hat{\theta}_{2}$ in equation (5.11). ($c^{*}$ will be a function of the variances and the covariance of the estimators.)

$$
\begin{array}{l}
\operatorname{var}(\hat{\theta}_{c})&=\operatorname{var}\left(c \hat{\theta}_{1}+(1-c)\hat{\theta}_{2}\right)\\
&=\operatorname{var}\left(c\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)+\hat{\theta}_{2}\right) \\
&=c^{2} \operatorname{var}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)+\operatorname{var}\left(\hat{\theta}_{2}\right)+2 c \operatorname{cov}\left(\hat{\theta}_{1}-\hat{\theta}_{2}, \hat{\theta}_{2}\right) \\
&=c^{2} \operatorname{var}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)+\frac{\left[\operatorname{cov}\left(\hat{\theta}_{1}-\hat{\theta}_{2}, \hat{\theta}_{2}\right)\right]^{2}}{\operatorname{var}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)}+2\frac{c\operatorname{cov}\left(\hat{\theta}_{1}-\hat{\theta}_{2},\hat{\theta}_{2}\right)}{\operatorname{var}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)}-\frac{\left[\operatorname{cov}\left(\hat{\theta}_{1}-\hat{\theta}_{2},\hat{\theta}_{2}\right)\right]^{2}}{\operatorname{var}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)}+\operatorname{var}\left(\hat{\theta}_{2}\right) \\
&=\left(\operatorname{csd}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)+\frac{\operatorname{cov}\left(\hat{\theta}_{1}-\hat{\theta}_{2},\hat{\theta}_{2}\right)}{sd\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)}\right)^{2}-\frac{\left[\operatorname{cov}\left(\hat{\theta}_{1}-\hat{\theta}_{2},\hat{\theta}_{2}\right)\right]^{2}}{\hat{\operatorname{var}}\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)}+\operatorname{var}\left(\hat{\theta}_{2}\right)
\end{array}
$$
## HW3

### EX5.13
Find two importance functions f1 and f2 that are supported on (1, ∞) and are ‘close’ to
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1.
$$
Which of your two importance functions should produce the smaller variance in estimating
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling? Explain.

```{r HW3}
set.seed(234)
x <- seq(1, 5, .01);w <- 2
g <- x^2 * exp(-x^2/2) / sqrt(2*pi)
y1 <- (x-1) * exp(-(x-1)^2/2) 
y2 <- x^0.5 * exp(-x) / (sqrt(pi)/2)

par(mfrow=c(1,2))
#figure (a)
plot(x, g, type = "l", ylab = "",
     ylim = c(0,0.5), lwd = w,col=1,main='(A)')
lines(x, y1, lty = 3, lwd = w,col=2)
lines(x, y2, lty = 4, lwd = w,col=3)

#figure (b)
plot(x, g/y1, type = "l", ylab = "",
     ylim = c(0,1.5), lwd = w, lty = 2,col=2,main='(B)')
lines(x, g/y2, lty = 3, lwd = w,col=3)

## estimate the integral
est <- sd <- numeric(2)
u <- runif(1e4) #using y1
x <- 1+sqrt(-2*log(1-u))
g <- function(x) {
  (x-1)^2 * exp(-(x-1)^2/2) / sqrt(2*pi)
}
fg <- g(x) / ((x-1) * exp(-(x-1)^2/2))
est[1] <- mean(fg)
sd[1] <- sd(fg)

x <- rgamma(1e4,shape=1.5,scale=1) #using y2
g <- function(x) {
      x^2 * exp(-x^2/2) / sqrt(2*pi)
    }
fg <- g(x) / dgamma(x,shape=1.5,scale=1)
est[2] <- mean(fg)
sd[2] <- sd(fg) 

rbind(est, sd)
    

```

As the result shows, function $y_{1}$ has a smaller variance.

### EX5.15

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

```{r}
M <- 10000; k <- 5 
r <- M/k 
N <- 50 #number of times to repeat the estimation
T2 <- numeric(k)
est <- matrix(0, N, 2)
g<-function(x)exp(-x)/(1+x^2)*(x>0)*(x<1)
for (i in 1:N) {
  est[i, 1] <- mean(g(runif(M)))
  for(j in 1:k){
    u <- runif(r, 0, 1)
    x <- -log(1-u)*(exp(-(j-1)/5)-exp(-j/5))
    T2[j] <- mean((exp(-(j-1)/5)-exp(-j/5))/(1+x^2))
    }
  est[i, 2] <- 5*mean(T2)
}
apply(est,2,mean)
```


### EX6.4

Suppose that $X1, . . . , Xn$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $µ$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

first generate $X_{1},...X_{n}$ from a LN(1, 2) distribution, n =20, after change of variables as $Y=ln(X)$, we have Y ~ N(1, 2), so the confidence level is exactly
$$
P\left(t_{\alpha / 2}(n-1)<\frac{\sqrt{n}(\bar{X}-\mu)}{S}<t_{1-\alpha/2}(n-1)\right)=1-\alpha
$$
first generate $X_{1},...X_{n}$ from a LN(1, 2) distribution, n =20, after change of variables as $Y=ln(X)$, we have Y ~ N(1, 2), so the confidence level is exactly
$$
P\left(t_{\alpha / 2}(n-1)<\frac{\sqrt{n}(\bar{X}-\mu)}{S}<t_{1-\alpha/2}(n-1)\right)=1-\alpha
$$

```{r}
n <- 20
alpha <- .05
n <- 20
alpha <- .05
set.seed(123)
UCL <- replicate(1000, expr = {
  x <- rlnorm(n, meanlog = 1, sdlog = 2)
  y <- log(x)
  sd(y) * qt(0.95, df = n-1) / sqrt(n) + mean(y)
} )
mean(UCL > 1) ## the estimated confidence level

calcCI <- function(n, alpha) {
  x <- rlnorm(n, meanlog = 1, sdlog = 2)
  y <- log(x)
return(sd(y) * qt(alpha, df = n-1) / sqrt(n) + mean(y))
}
C1 <- replicate(1000, expr = calcCI(n = 20, alpha = .025))
C2 <- replicate(1000, expr = calcCI(n = 20, alpha = .975))
print(c(exp(mean(C1)), exp(mean(C2)))) ## the confidence interval
```

So we can see, empirical estimate of the confidence level is 0.948. The 95% confidence interval for the parameter $µ$ is (1.06, 7.08).

### EX6.5

Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^{2}(2)$ data with sample size $n = 20$. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

```{r}
n <- 20
alpha <- .05
set.seed(123)
VCL <- replicate(1000, expr = {
  x <- rchisq(n, df = 2)
  sd(x) * qt(1-alpha, df = n-1) / sqrt(n) + mean(x)
} )
mean(VCL > 2)
```


## HW4

### EX6.7

Estimate the power of the skewness test of normality against symmetric $Beta(α, α)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(ν)$?

```{r HW4}
#first write a function to compute the sample skewness coeff
sk <- function(x) {
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return( m3 / m2^1.5 )
}

alpha <- 0.1
n <- 30
m <- 2500
#critical value for the skewness test
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3)))) 
a <- seq(1,20,.1)
N <- length(a)
pwr <- pwr1 <- numeric(N)
for (j in 1:N) { #for each epsilon
  a1 <- a[j]
  sktests <- sktests1 <- numeric(m)
  for (i in 1:m) { #for each replicate
    x <- rbeta(n, a1, a1)
    sktests[i] <- as.integer(abs(sk(x)) >= cv)
    y <- rt(n, a1)
    sktests1[i] <- as.integer(abs(sk(y)) >= cv)
  }
pwr[j] <- mean(sktests)
pwr1[j] <- mean(sktests1)
}

```

```{r}
par(mfrow=c(1,2))
#plot power vs a
plot(a, pwr, type = "b",
xlab = bquote(a), ylim = c(0,0.14))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(a, pwr+se, lty = 3)
lines(a, pwr-se, lty = 3)
#plot power vs a
plot(a, pwr1, type = "b",
xlab = bquote(a), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr1 * (1-pwr1) / m) #add standard errors
lines(a, pwr1+se, lty = 3)
lines(a, pwr1-se, lty = 3)
```

```{r}
# t(v)
pwr_t = function(v){
 
 alpha = 0.1
 n = 20
 m = 1e3
 N = length(v)
 pwr = numeric(N)
 cv = qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
 
 for (j in 1:N) { 
  sktests = numeric(m)
  for (i in 1:m) { 
   x = rt(n,v[j])
   sktests[i] = as.integer(abs(sk(x))>= cv)
  }
  pwr[j] = mean(sktests)
 }
 se = sqrt(pwr*(1-pwr) / m) 
  return(list(pwr = pwr,se = se))
}

v = seq(1,20)
pwr = pwr_t(v)$pwr
se = pwr_t(v)$se
# plot the power
plot(v, pwr, type = "b", xlab = "v", ylab = "pwr", ylim = c(0,1),pch=16)
abline(h = 0.1, lty = 2)
lines(v, pwr+se, lty = 4)
lines(v, pwr-se, lty = 4)
```

For t distribution, the empirical power is always bigger than 0.1 and it decreases to 0.1 with the increase of v.

### EX6.8

Refer to Example 6.16. Repeat the simulation, but also compute the F test of equal variance, at significance level $\hat{\alpha} \doteq 0.055$. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)

```{r}
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}

m <- 1e4

n <- c(20, 50, 100)
power <- F.test <- numeric(length(n))
for (i in 1:length(n)){
  sigma1 <- 1
  sigma2 <- 1.5
  power[i] <- mean(replicate(m, expr={
    x <- rnorm(n[i], 0, sigma1)
    y <- rnorm(n[i], 0, sigma2)
    count5test(x, y)
  }))
  F.test[i] <- mean(replicate(m, expr={
    x <- rnorm(n[i], 0, sigma1)
    y <- rnorm(n[i], 0, sigma2)
    var.test(x, y, conf.level = 0.055)$estimate
  }))
}
res <- rbind(power=round(power,3), F.test=round(F.test,3))
res
```


### EX6.C

Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as
$$
\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}.
$$
Under normality, $\beta_{1, d}=0$. The multivariate skewness statistic is
$$
b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3},
$$
```{r}
Mardia.test <- function(X) {
  n <- nrow(X)
  c <- ncol(X)
  mydata <- X
  for(i in 1:c){
    mydata[,i]<-X[,i]-mean(X[,i])
  }
  sigmah<-t(mydata)%*%mydata/n
  a<-mydata%*%solve(sigmah)%*%t(mydata)
  b<-sum(colSums(a^{3}))/(n*n)
  test<-n*b/6
  chi<-qchisq(0.95,c*(c+1)*(c+2)/6)
  as.integer(test>chi)
}

library(MASS)
set.seed(234)
mu <- c(0,0,0)
sigma <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
m=1000
n<-c(10, 20, 30, 50, 100, 500)
p.reject=numeric(length(n))
for(i in 1:length(n)){
  p.reject[i]=mean(replicate(m, expr={
    X <- mvrnorm(n[i],mu,sigma) 
    Mardia.test(X)
  }))
}
print(p.reject)
```

```{r}
## refer to TA
library(MASS)
set.seed(7912)
set.seed(7912)
mu1 <- mu2 <- c(0,0,0)
sigma1 <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
sigma2 <- matrix(c(100,0,0,0,100,0,0,0,100),nrow=3,ncol=3)
sigma=list(sigma1,sigma2)
m=1000
n=50
#m: number of replicates; n: sample size
epsilon <- c(seq(0, .06, .01), seq(.1, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
for (j in 1:N) { #for each epsilon
  e <- epsilon[j]
  sktests <- numeric(m)
  for (i in 1:m) { #for each replicate
    index=sample(c(1, 2), replace = TRUE, size = n, prob = c(1-e, e))
    mydata<-matrix(0,nrow=n,ncol=3)
    for(t in 1:n){
      if(index[t]==1) mydata[t,]=mvrnorm(1,mu1,sigma1) 
      else mydata[t,]=mvrnorm(1,mu2,sigma2)
    }
    sktests[i] <- Mardia.test(mydata)
  }
  pwr[j] <- mean(sktests)
}
plot(epsilon, pwr, type = "b",
     xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .05, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```

## HW5

### EX7.1

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

```{r HW5}
## jackknife estimate of the bias and standard error
b.cor <- function(x,i) cor(x[i,1],x[i,2])
data(law, package = "bootstrap")
x <- as.matrix(law)
n <- nrow(law)
theta.hat <- cor(law$LSAT, law$GPA)
theta.jack <- numeric(n)
for(i in 1:n){
  theta.jack[i] <- b.cor(x,(1:n)[-i])
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2))
round(c(original=theta.hat, bias.jack=bias.jack, se.jack=se.jack), 3)
```



### EX7.5

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/ \lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ. 

```{r echo=FALSE}
boot.BCa <-
function(x, th0, th, stat, conf = .95) {
# bootstrap with BCa bootstrap confidence interval
# th0 is the observed statistic
# th is the vector of bootstrap replicates
# stat is the function to compute the statistic
x <- as.matrix(x)
n <- nrow(x) #observations in rows
N <- 1:n
alpha <- (1 + c(-conf, conf))/2
zalpha <- qnorm(alpha)
# the bias correction factor
z0 <- qnorm(sum(th < th0) / length(th))
# the acceleration factor (jackknife est.)
th.jack <- numeric(n)
for (i in 1:n) {
J <- N[1:(n-1)]
th.jack[i] <- stat(x[-i, ], J)
}
L <- mean(th.jack) - th.jack
a <- sum(L^3)/(6 * sum(L^2)^1.5)
# BCa conf. limits
adj.alpha <- pnorm(z0 + (z0+zalpha)/(1-a*(z0+zalpha)))
limits <- quantile(th, adj.alpha, type=6)
return(list("est"=th0, "BCa"=limits))
}
```


```{r}
library(boot) #for boot and boot.ci
data("aircondit", package = "boot")
dat <- as.matrix(aircondit)
theta.hat <- mean(dat)
theta.boot <- function(dat, ind) {
  x <- dat[ind, 1]
  #function to compute the statistic
  mean(x) 
}
boot.obj <- boot(dat, statistic = theta.boot, R = 2000)

alpha <- c(.025, .975)
#normal
print(boot.obj$t0 + qnorm(alpha) * sd(boot.obj$t))
#basic
print(2*boot.obj$t0 - quantile(boot.obj$t, rev(alpha), type=1))
#percentile
print(quantile(boot.obj$t, alpha, type=6))
#BCa
boot.BCa(x, th0 = mean(dat), th = boot.obj$t, stat = theta.boot)

```

the sampling distribution of the statistics is not close to normal, so the results are different.

### EX7.8

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

```{r}
data("scor", package = "bootstrap")
ev <- eigen(cov(scor))$values
theta.hat <- ev[1]/sum(ev)
x <- as.matrix(scor)
n <- nrow(scor)
theta.jack <- numeric(n)
for(i in 1:n){
  x1 <- x[-i, ]
  ev <- eigen(cov(x1))$values
  theta.jack[i] <- ev[1]/sum(ev)
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2))
round(c(original=theta.hat, bias.jack=bias.jack, se.jack=se.jack), 3)
```


### EX7.11

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lattice) 
library(DAAG)
attach(ironslag)
```

```{r}
data("ironslag", package = "DAAG")
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n-1)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n-1) {
  y <- magnetic[-c(k,k+1)]
  x <- chemical[-c(k,k+1)]
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
  e1[k] <- magnetic[k] - yhat1
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
    J2$coef[3] * chemical[k]^2
  e2[k] <- magnetic[k] - yhat2
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
  yhat3 <- exp(logyhat3)
  e3[k] <- magnetic[k] - yhat3
  J4 <- lm(log(y) ~ log(x))
  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
  yhat4 <- exp(logyhat4)
  e4[k] <- magnetic[k] - yhat4
}
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```

According to the prediction error criterion, Model 2, the quadratic model, is still the best fit for the data.

## HW6

### EX8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

```{r HW6}
maxout <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(max(c(outx, outy)))
}

n1 <- 20
n2 <- 50
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 1000
# generate samples under H0
set.seed(123)
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
t0 <- replicate(m, expr={
  x <- rnorm(n1, mu1, sigma1)
  y <- rnorm(n2, mu2, sigma2)
  x <- x - mean(x)
  y <- y - mean(y)
  maxout(x, y)
}) ## statistics before permutation

stat1 <- replicate(m, expr={
  x <- rnorm(n1, mu1, sigma1)
  y <- rnorm(n2, mu2, sigma2)
  z <- c(x, y)
  K <- 1:(n1 + n2)
  k <- sample(K, size = n1, replace = FALSE)
  x1 <- z[k];y1 <- z[-k] #complement of x1
  maxout(x1, y1)
}) ## statistics after permutation

p.value <- mean(stat1 >= t0)
p.value
```

### Design experiments

Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations.
I Unequal variances and equal expectations
I Unequal variances and unequal expectations
I Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)
I Unbalanced samples (say, 1 case versus 10 controls)
I Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).

```{r}
library(RANN)
library(boot)
library(Ball)
library(energy)
library(MASS)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1] 
  block2 <- NN$nn.idx[(n1+1):n,-1] 
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5) 
  (i1 + i2) / (k * n)
}
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
  sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}

```

```{r}
mu1 <- c(0,0,0)
sigma1 <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
mu2 <- c(0,0,0)
sigma2 <- matrix(c(2,0,0,0,3,0,0,0,4),nrow=3,ncol=3)
n1=n2=20
n <- n1+n2 
N = c(n1,n2)
k=3
R=999
m=100
set.seed(1234)
p.values <- matrix(NA,m,3)
for(i in 1:m){
  mydata1 <- mvrnorm(n1,mu1,sigma1)
  mydata2 <- mvrnorm(n2,mu2,sigma2)
  mydata <- rbind(mydata1,mydata2)
  p.values[i,1] <- eqdist.nn(mydata,N,k)$p.value
  p.values[i,2] <- eqdist.etest(mydata,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=mydata1,y=mydata2,num.permutations=R,seed=i*2846)$p.value
}
alpha <- 0.05;
pow1 <- colMeans(p.values<alpha)
pow1
```

```{r}
mu1 <- c(0,0,0)
sigma1 <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
mu2 <- c(0.5,-0.5,0.5)
sigma2 <- matrix(c(2,0,0,0,2,0,0,0,2),nrow=3,ncol=3)
n1=n2=20
n <- n1+n2 
N = c(n1,n2)
k=3
R=999
m=100
set.seed(1234)
p.values <- matrix(NA,m,3)
for(i in 1:m){
  mydata1 <- mvrnorm(n1,mu1,sigma1)
  mydata2 <- mvrnorm(n2,mu2,sigma2)
  mydata <- rbind(mydata1,mydata2)
  p.values[i,1] <- eqdist.nn(mydata,N,k)$p.value
  p.values[i,2] <- eqdist.etest(mydata,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=mydata1,y=mydata2,num.permutations=R,seed=i*2846)$p.value
}
alpha <- 0.05;
pow2 <- colMeans(p.values<alpha)
pow2
```

```{r}
n1=n2=20
n <- n1+n2 
N = c(n1,n2)
k=3
R=999
m=100
set.seed(1234)
p.values <- matrix(NA,m,3)
for(i in 1:m){
  mydata1 <- as.matrix(rt(n1,1,2),ncol=1)
  mydata2 <- as.matrix(rt(n2,2,5),ncol=1)
  mydata <- rbind(mydata1,mydata2)
  p.values[i,1] <- eqdist.nn(mydata,N,k)$p.value
  p.values[i,2] <- eqdist.etest(mydata,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=mydata1,y=mydata2,num.permutations=R,seed=i*2846)$p.value
}
alpha <- 0.05;
pow3 <- colMeans(p.values<alpha)
pow3
```

```{r}
n1=n2=20
n <- n1+n2 
N = c(n1,n2)
k=3
R=999
m=100
set.seed(1234)
p.values <- matrix(NA,m,3)
rbimodel<-function(n,mu1,mu2,sd1,sd2){
  index=sample(1:2,n,replace=TRUE)
  x=numeric(n)
  index1<-which(index==1)
  x[index1]<-rnorm(length(index1), mu1, sd1)
  index2<-which(index==2)
  x[index2]<-rnorm(length(index2), mu2, sd2)
  return(x)
}
for(i in 1:m){
  mydata1 <- as.matrix(rbimodel(n1,0,0,1,2),ncol=1)
  mydata2 <- as.matrix(rbimodel(n2,1,1,4,3),ncol=1)
  mydata <- rbind(mydata1,mydata2)
  p.values[i,1] <- eqdist.nn(mydata,N,k)$p.value
  p.values[i,2] <- eqdist.etest(mydata,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=mydata1,y=mydata2,num.permutations=R,seed=i*2846)$p.value
}
alpha <- 0.05;
pow4 <- colMeans(p.values<alpha)
pow4
```

```{r}
mu1 <- c(0,0,0)
sigma1 <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
mu2 <- c(0.5,-0.5,0.5)
sigma2 <- matrix(c(2,0,0,0,2,0,0,0,2),nrow=3,ncol=3)
n1=10
n2=100
n <- n1+n2 
N = c(n1,n2)
k=3
R=999
m=100
set.seed(1234)
p.values <- matrix(NA,m,3)
for(i in 1:m){
  mydata1 <- mvrnorm(n1,mu1,sigma1)
  mydata2 <- mvrnorm(n2,mu2,sigma2)
  mydata <- rbind(mydata1,mydata2)
  p.values[i,1] <- eqdist.nn(mydata,N,k)$p.value
  p.values[i,2] <- eqdist.etest(mydata,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=mydata1,y=mydata2,num.permutations=R,seed=i*2846)$p.value
}
alpha <- 0.05;
pow5 <- colMeans(p.values<alpha)
pow5
```

## HW7

### EX1

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

The standard Laplace distribution has density $f(x)=\frac{1}{2} e^{-|x|}, x \in \mathbb{R}$

```{r}
f <- function(x) {
  return(exp(-abs(x))/2)
}
```


```{r}
MetropolisR <- function(n, sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (f(y) / f(x[i-1])))
      x[i] <- y else {
        x[i] <- x[i-1]
        k <- k + 1
      }
  }
  return(list(x=x, k=k))
}

n <- 4 #degrees of freedom for target Student t dist.
N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 25
rw1 <- MetropolisR(n, sigma[1], x0, N)
rw2 <- MetropolisR(n, sigma[2], x0, N)
rw3 <- MetropolisR(n, sigma[3], x0, N)
rw4 <- MetropolisR(n, sigma[4], x0, N)
#number of candidate points rejected

print(1-c(rw1$k, rw2$k, rw3$k, rw4$k)/N) ## acceptance rate

plot(rw1$x[-(1:500)],type="l")
plot(rw2$x[-(1:500)],type="l")
plot(rw3$x[-(1:500)],type="l")
plot(rw4$x[-(1:500)],type="l")

```


### EX2

For Exercise 9.4, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$.

The target distribution is the standard Laplace distribution, and the proposal distribution is Normal(Xt, σ2).

```{r}

ldf <- function(x,u,b) exp(-abs(x-u)/b)/2/b

Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic
  return(r.hat)
}

normal.chain <- function(sigma, N, X1) {
  #generates a Metropolis chain for the standard Laplace distribution
  #with Normal(X[t], sigma) proposal distribution
  #and starting value X1
  x <- rep(0, N)
  x[1] <- X1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rnorm(1, xt, sigma) #candidate point
    r1 <- ldf(y, 0, 1) * dnorm(xt, y, sigma)
    r2 <- ldf(xt, 0, 1) * dnorm(y, xt, sigma)
    r <- r1 / r2
    if (u[i] <= r) x[i] <- y else
      x[i] <- xt
  }
  return(x)
}
```

```{r}
sigma <- 2 #parameter of proposal distribution
k <- 4 #number of chains to generate
n <- 2000 #length of chains
b <- 1000 #burn-in length
#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ] <- normal.chain(sigma, n, x0[i])
#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
cat("R_hat = " ,Gelman.Rubin(psi)) 
```


### EX3

Find the intersection points $A(k)$ in $(0, \sqrt{k})$ of the curves
$$
S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^{2}(k-1)}{k-a^{2}}}\right)
$$
and
$$
S_{k}(a)=P\left(t(k)>\sqrt{\frac{a^{2} k}{k+1-a^{2}}}\right)
$$
for k = 4 : 25, 100, 500, 1000, where t(k) is a Student t random variable with k degrees of freedom. (These intersection points determine the critical values for a t-test for scale-mixture errors proposed by Sz´ekely [260].)

```{r}
g <- function(a){
  tmp1 <- dt(a^2*(k-1)/(k-a^2),k-1)
  tmp2 <- dt(a^2*(k)/(k+1-a^2),k)
  tmp1 - tmp2
}
solution <- numeric(25)
for (k in c(4:25,100,500,1000)){
  solution <- uniroot(g,c(0,6))
  cat(k, round(as.numeric(solution),5),"\n")
}

```

## HW8

### EX1

Observed data likelihood:
$L\left(p ,q\mid n_{A}., n_{B}.,n_{OO},n_{AB}\right)=\left(p^{2}+2pr\right)^{n_{A}.}\left(q^{2}+2qr\right)^{n_{B}.}\left(r^{2}\right)^{n_{OO}}\left(2pq\right)^{n_{AB}} \Rightarrow p=0.3, q=0.1, r=0.6$
Complete data likelihood:
$L\left(p ,q\mid n_{A}., n_{B}.,n_{OO},n_{AB},n_{AA},n_{BB}\right)=\left(p^{2}\right)^{n_{AA}}\left(q^{2}\right)^{n_{BB}}\left(r^{2}\right)^{n_{OO}}\left(2pr\right)^{n_{A}.-n_{AA}}\left(2qr\right)^{n_{B}.-n_{BB}}\left(2pq\right)^{n_{AB}}$
then,
$$I\left(p ,q\mid n_{A}., n_{B}.,n_{OO},n_{AB},n_{AA},n_{BB}\right)=n_{A A} \log (p/r)+n_{B B} \log (q/r)+2 n_{OO} \log (r)+n_{A}. \log (pr)+n_{B}. \log (qr)+n_{AB} \log (pq)$$

```{r}
est <- function(ep = 1e-7){
  p = 0.2; q = 0.1
  k = 1
  repeat{
    k = k+1
    p[k]=(444*p[k-1]/(2-p[k-1]-2*q[k-1])+507)/2000
    q[k]=(132*q[k-1]/(2-q[k-1]-2*p[k-1])+195)/2000
    if(abs(p[k]-p[k-1])<ep | abs(q[k]-q[k-1])<ep)break
  }
  list(p=p[k], q=q[k], iter=k)
}
est()
```
 

### EX2

Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
```{r}
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
```

```{r}
## use for loops
attach(mtcars) 
formulas <- as.character(formulas)
for (i in 1:length(formulas)){
  lm <- lm(formulas[i])
  print(lm)
}
```

```{r}
## use lapply()
lapply(formulas, function(x) lm(x))
```


### EX3

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
```{r}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
```
Extra challenge: get rid of the anonymous function by using [[ directly.

```{r}
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
sapply(trials, function(f) f$p.value)
sapply(trials, "[[", 3)

```


### EX4

Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

```{r}
set.seed(123)
xs <- list(runif(10), runif(20)) # different element lengths
ws <- list(rpois(10, 2) + 1, rpois(20, 2) + 1)
nlapply <- function(x1,x2, FUN, FUN.VALUE, USE.NAMES = TRUE){
  answer <- Map(FUN, x1,x2)
  vapply(answer, function(x) x, FUN.VALUE = FUN.VALUE)
}
nlapply(xs,ws,weighted.mean, numeric(1)) 

```

## HW9

### Question
 Write an Rcpp function for Exercise 9.4(page 277,Statistical Computing with R)
 Compare the corresponding generated random numbers with those by the R function you wrote before using the function "qqplot".
 Campare the computation time of the two functions with the function "microbenthmark".
 Comments your results.
 
```{r}
library(StatComp20073)
library(Rcpp)
dir_cpp <- '../src/'
sourceCpp(paste0(dir_cpp,"MetropolisC.cpp")) 
n <- 4 #degrees of freedom for target Student t dist.
N <- 2000
sigma <- 2
x0 <- 25
set.seed(234)
MR <- MetropolisR(n,sigma,x0,N)$x[-(1:500)]
MC <- MetropolisC(sigma,x0,N)[-(1:500)]
qqplot(MC, MR)
abline(a=0,b=1,col='black')
```

```{r}
library(microbenchmark)
times <- microbenchmark(MC=MetropolisC(sigma,x0,N), MR=MetropolisR(n,sigma,x0,N))
t <- summary(times)
t
```

###Rcpp is more efficient than R when a loop is needed.

